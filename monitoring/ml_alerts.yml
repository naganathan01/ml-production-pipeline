groups:
- name: ml_model_alerts
  rules:
  # High error rate alert
  - alert: HighMLErrorRate
    expr: rate(ml_prediction_errors_total[5m]) > 0.1
    for: 2m
    labels:
      severity: critical
      service: ml-model
    annotations:
      summary: "High ML prediction error rate"
      description: "ML model error rate is {{ $value }} errors per second"

  # High latency alert
  - alert: HighMLLatency
    expr: histogram_quantile(0.95, rate(ml_prediction_duration_seconds_bucket[5m])) > 1.0
    for: 5m
    labels:
      severity: warning
      service: ml-model
    annotations:
      summary: "High ML prediction latency"
      description: "95th percentile latency is {{ $value }} seconds"

  # Low prediction volume (could indicate issues)
  - alert: LowMLPredictionVolume
    expr: rate(ml_predictions_total[10m]) < 0.1
    for: 10m
    labels:
      severity: warning
      service: ml-model
    annotations:
      summary: "Low ML prediction volume"
      description: "Prediction rate is only {{ $value }} per second"

  # Model service down
  - alert: MLServiceDown
    expr: up{job="ml-model-api"} == 0
    for: 1m
    labels:
      severity: critical
      service: ml-model
    annotations:
      summary: "ML service is down"
      description: "ML model service has been down for more than 1 minute"

  # High memory usage
  - alert: HighMLMemoryUsage
    expr: container_memory_usage_bytes{pod=~"ml-model-.*"} / container_spec_memory_limit_bytes > 0.9
    for: 5m
    labels:
      severity: warning
      service: ml-model
    annotations:
      summary: "High memory usage in ML pods"
      description: "Memory usage is {{ $value | humanizePercentage }} of limit"

  # Pod restart alert
  - alert: MLPodRestarting
    expr: rate(kube_pod_container_status_restarts_total{pod=~"ml-model-.*"}[15m]) > 0
    for: 5m
    labels:
      severity: warning
      service: ml-model
    annotations:
      summary: "ML pod is restarting frequently"
      description: "Pod {{ $labels.pod }} is restarting at rate {{ $value }} per minute"